{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5437be1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load env variables and create client\n",
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Anthropic()\n",
    "model = \"claude-3-5-haiku-latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b0d8e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def add_user_message(messages, text):\n",
    "    user_message = {\"role\": \"user\", \"content\": text}\n",
    "    messages.append(user_message)\n",
    "\n",
    "\n",
    "def add_assistant_message(messages, text):\n",
    "    assistant_message = {\"role\": \"assistant\", \"content\": text}\n",
    "    messages.append(assistant_message)\n",
    "\n",
    "\n",
    "def chat(messages, system=None, temperature=1.0, stop_sequences=[]):\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop_sequences\": stop_sequences,\n",
    "    }\n",
    "\n",
    "    if system:\n",
    "        params[\"system\"] = system\n",
    "\n",
    "    message = client.messages.create(**params)\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e788701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def generate_dataset():\n",
    "    prompt = \"\"\"\n",
    "Generate a evaluation dataset for a prompt evaluation. The dataset will be used to evaluate prompts\n",
    "that generate Python, JSON, or Regex specifically for AWS-related tasks. Generate an array of JSON objects,\n",
    "each representing task that requires Python, JSON, or a Regex to complete. Also, you should include a solution criteria, explaining what an awesome\n",
    "solution would look like. You should think about tasks that are relevant for AWS developers.\n",
    "\n",
    "Example output:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"task\": \"Description of task\",\n",
    "        \"format\": \"python\" or \"json\" or \"regex\",\n",
    "        \"solution_criteria\": \"Description of what an awesome solution would look like\"\n",
    "    },\n",
    "    ...additional\n",
    "]\n",
    "```\n",
    "\n",
    "* Focus on tasks that can be solved by writing a single Python function, a single JSON object, or a regular expression.\n",
    "* Focus on tasks that do not require writing much code\n",
    "\n",
    "Please generate 4 objects.\n",
    "\"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, prompt)\n",
    "    add_assistant_message(messages, \"```json\")\n",
    "\n",
    "    answer = chat(messages, stop_sequences=[\"```\"])\n",
    "    return json.loads(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75596f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataset and store it on a json file\n",
    "dataset = generate_dataset()\n",
    "\n",
    "with open(\"dataset.json\", \"w\") as f:\n",
    "    json.dump(dataset, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b329b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt(test_case):\n",
    "    \"\"\"Merges the prompt and test case input, then returns the result\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert in AWS programmer, your task is to solve the following task:\n",
    "\n",
    "    {test_case[\"task\"]}\n",
    "\n",
    "    <output>\n",
    "    You should only return JSON, Python or Regex code as output.\n",
    "    Do NOT add any comments or commentary explanation.\n",
    "    </output>\n",
    "    \"\"\"\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    add_user_message(messages, prompt)\n",
    "    add_assistant_message(messages, \"```code\")\n",
    "\n",
    "    output = chat(messages, stop_sequences=[\"```\"])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0628733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_by_model(test_case, output):\n",
    "    eval_prompt = f\"\"\"\n",
    "    You are an expert AWS code reviewer. Your task is to evaluate the following AI-generated solution.\n",
    "\n",
    "    Original Task:\n",
    "    <task>\n",
    "    {test_case[\"task\"]}\n",
    "    </task>\n",
    "\n",
    "    Solution to Evaluate:\n",
    "    <solution>\n",
    "    {output}\n",
    "    </solution>\n",
    "\n",
    "    Criteria to use for Evaluation:\n",
    "    <solution_criteria>\n",
    "    {test_case[\"solution_criteria\"]}\n",
    "    </solution_criteria>\n",
    "    \n",
    "    Output Format\n",
    "    Provide your evaluation as a structured JSON object with the following fields, in this specific order:\n",
    "    - \"strengths\": An array of 1-3 key strengths\n",
    "    - \"weaknesses\": An array of 1-3 key areas for improvement\n",
    "    - \"reasoning\": A concise explanation of your overall assessment\n",
    "    - \"score\": A number between 1-10\n",
    "\n",
    "    Respond with JSON. Keep your response concise and direct.\n",
    "    Example response shape:\n",
    "    {{\n",
    "        \"strengths\": string[],\n",
    "        \"weaknesses\": string[],\n",
    "        \"reasoning\": string,\n",
    "        \"score\": number\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    add_user_message(messages, eval_prompt)\n",
    "\n",
    "    add_assistant_message(messages, \"```json\")\n",
    "\n",
    "    eval_text = chat(messages, stop_sequences=[\"```\"])\n",
    "\n",
    "    return json.loads(eval_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a1d891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_regex(text):\n",
    "    try:\n",
    "        import re\n",
    "        re.compile(text.strip())\n",
    "        return 10\n",
    "    except re.error:\n",
    "        return 0\n",
    "    \n",
    "def validate_json(text):\n",
    "    try:\n",
    "        json.loads(text.strip())\n",
    "        return 10\n",
    "    except json.JSONDecodeError:\n",
    "        return 0\n",
    "    \n",
    "def validate_python(text):\n",
    "    try:\n",
    "        import ast\n",
    "        ast.parse(text.strip())\n",
    "        return 10\n",
    "    except SyntaxError:\n",
    "        return 0\n",
    "\n",
    "def grade_syntax(response, test_case):\n",
    "    if test_case[\"format\"] == \"regex\":\n",
    "        return validate_regex(response)\n",
    "    elif test_case[\"format\"] == \"json\":\n",
    "        return validate_json(response)\n",
    "    elif test_case[\"format\"] == \"python\":\n",
    "        return validate_python(response)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e250dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_case(test_case):\n",
    "    \"\"\"Calls run_prompt, then grades the result\"\"\"\n",
    "    output = run_prompt(test_case)\n",
    "\n",
    "    model_grade = grade_by_model(test_case, output)\n",
    "    code_grade = grade_syntax(output, test_case)\n",
    "    score = model_grade[\"score\"]\n",
    "    reasoning = model_grade[\"reasoning\"]\n",
    "\n",
    "    return {\n",
    "        \"output\": output,\n",
    "        \"test_case\": test_case,\n",
    "        \"final_score\": (score + code_grade) / 2,\n",
    "        \"model_score\": score,\n",
    "        \"code_score\": code_grade,\n",
    "        \"reasoning\": reasoning\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17a8d0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def run_eval(dataset):\n",
    "    \"\"\"Loads the dataset and calls run_test_case with each case\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for test_case in dataset:\n",
    "        result = run_test_case(test_case)\n",
    "        results.append(result)\n",
    "\n",
    "    average_score = mean([result[\"final_score\"] for result in results])\n",
    "    print(average_score)    \n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20a5639f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.375\n"
     ]
    }
   ],
   "source": [
    "# Run eval pipeline\n",
    "\n",
    "with open(\"dataset.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "results = run_eval(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f2e7dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"output\": \"\\n{\\n    \\\"Version\\\": \\\"2012-10-17\\\",\\n    \\\"Statement\\\": [\\n        {\\n            \\\"Effect\\\": \\\"Allow\\\",\\n            \\\"Action\\\": [\\n                \\\"s3:ListBucket\\\",\\n                \\\"s3:GetObject\\\"\\n            ],\\n            \\\"Resource\\\": [\\n                \\\"arn:aws:s3:::company-data\\\",\\n                \\\"arn:aws:s3:::company-data/*\\\"\\n            ]\\n        }\\n    ]\\n}\\n\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Create a JSON policy that restricts an IAM user to only list and read objects in a specific S3 bucket named 'company-data'\",\n",
      "      \"format\": \"json\",\n",
      "      \"solution_criteria\": \"Policy should use least privilege principles, explicitly allow s3:ListBucket and s3:GetObject actions only on the specified bucket, and deny all other S3 actions\"\n",
      "    },\n",
      "    \"final_score\": 8.5,\n",
      "    \"model_score\": 7,\n",
      "    \"code_score\": 10,\n",
      "    \"reasoning\": \"The policy provides basic read-only access to the company-data bucket but lacks comprehensive least privilege implementation. While it allows listing and reading objects, it does not explicitly prevent other potential S3 actions.\"\n",
      "  },\n",
      "  {\n",
      "    \"output\": \"\\ndef extract_aws_account_id(arn):\\n    \\\"\\\"\\\"\\n    Extract AWS Account ID from an ARN string\\n    \\n    Args:\\n        arn (str): AWS Resource Name (ARN)\\n    \\n    Returns:\\n        str: AWS Account ID\\n    \\\"\\\"\\\"\\n    import re\\n    \\n    # Regex pattern to match account ID in ARN\\n    account_id_pattern = r'arn:aws:.*:(\\\\d{12}):'\\n    \\n    # Search for account ID in ARN\\n    match = re.search(account_id_pattern, arn)\\n    \\n    if match:\\n        return match.group(1)\\n    else:\\n        return None\\n\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Write a Python function to extract the AWS account ID from an ARN string\",\n",
      "      \"format\": \"python\",\n",
      "      \"solution_criteria\": \"Function should use efficient string parsing or regex, handle different ARN formats, return the account ID as a string, and be concise (< 5 lines of code)\"\n",
      "    },\n",
      "    \"final_score\": 8.5,\n",
      "    \"model_score\": 7,\n",
      "    \"code_score\": 10,\n",
      "    \"reasoning\": \"The solution effectively extracts an AWS account ID using a regex pattern, but lacks comprehensive ARN validation. The implementation is concise and handles basic extraction, yet could be more robust in error handling and ARN format checking.\"\n",
      "  },\n",
      "  {\n",
      "    \"output\": \"_regex_lambda_function_name = r'^[a-zA-Z0-9-_]{1,64}$'\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Create a regex pattern to validate an AWS Lambda function name\",\n",
      "      \"format\": \"regex\",\n",
      "      \"solution_criteria\": \"Regex should enforce AWS Lambda function name rules: 1-64 characters, start with a letter, can contain letters, numbers, hyphens, and underscores\"\n",
      "    },\n",
      "    \"final_score\": 8.0,\n",
      "    \"model_score\": 6,\n",
      "    \"code_score\": 10,\n",
      "    \"reasoning\": \"The regex pattern partially meets AWS Lambda function name requirements. While it captures length and character type constraints, it misses the critical rule of starting with a letter. A more precise regex would enhance validation accuracy.\"\n",
      "  },\n",
      "  {\n",
      "    \"output\": \"_template\\ndef convert_cloudwatch_timestamp(log_timestamp):\\n    \\\"\\\"\\\"\\n    Convert CloudWatch log timestamp (milliseconds since epoch) to readable datetime\\n    \\n    Args:\\n        log_timestamp (int): Timestamp in milliseconds since epoch\\n    \\n    Returns:\\n        datetime: Converted datetime object\\n    \\\"\\\"\\\"\\n    from datetime import datetime\\n    \\n    # Convert milliseconds timestamp to datetime \\n    readable_datetime = datetime.fromtimestamp(log_timestamp / 1000.0)\\n    \\n    return readable_datetime\\n\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Write a Python function to convert CloudWatch log timestamps to readable datetime\",\n",
      "      \"format\": \"python\",\n",
      "      \"solution_criteria\": \"Function should efficiently convert millisecond timestamps to a human-readable datetime format, handle potential timestamp variations, and be compact and readable\"\n",
      "    },\n",
      "    \"final_score\": 8.5,\n",
      "    \"model_score\": 7,\n",
      "    \"code_score\": 10,\n",
      "    \"reasoning\": \"The solution provides a basic, functional approach to converting CloudWatch timestamps. It successfully translates millisecond timestamps to datetime objects, but lacks robustness for production use. The implementation is clean and concise, but needs additional error checking and timezone considerations.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(results, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-studies (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
